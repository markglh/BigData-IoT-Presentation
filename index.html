<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Big Data in IoT</title>

		<meta name="description" content="Big Data in IoT">
		<meta name="author" content="Mark Harrison">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/code.css">
		<link rel="stylesheet" href="reveal.js/css/reveal.css">
		<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<section>
					<h1>Architectural challenges of IoT and Big Data</h1>
					<h5>Plus a dose of Spark for good measure :)</h5>
					<p>
						<small>Created by <a href="https://github.com/markglh">Mark Harrison</a> / <a href="http://twitter.com/markglh">@markglh</a></small>
					</p>
				</section>

				<section>
					<img class="stretch" src="images/cake-solutions.png"/>
					<p>
						<small><a href="www.cakesolutions.net">www.cakesolutions.net</a></small>
					</p>
				</section>

				<section>
	          		<img class="stretch" src="images/assumptions.jpg"/>
				</section>

				<section>

					<section>
		          		<img class="stretch" src="images/meme-internet_all_the_things.png"/>
		          		<aside class="notes">
		          			<ul>
		          				<li>IoT is nothing new</li>
		          				<li>It's just a buzzword used to describe the connection of remote "things"</li>
		          				<li>The key idea is all those devices collect and exchange data over the internet</li>
		          			</ul>
		        		</aside>
					</section>

					<section>
		          		<img class="stretch" src="images/meme-wesellthose.jpg"/>
		          		<aside class="notes">
		          			<ul>
		          				<li>Everyone wants a piece of IoT</li>
		          				<li>Even if they don't know what they're actually selling</li>
		          				<li>No one size fits all solution</li>
		          				<li>No connectivity standards across devices</li>
		          				<li>Bluetooth, zigbee, z-wave, 6LowPAN, Thread, WiFi, NFC, Sigfox, Neul, LoRaWAN</li>
		          				<li>Because different devices have different use-cases!!</li>
		          				<li>Although some don't and should be standardised, samsung, lg, etc :|</li>
		          			</ul>
		        		</aside>
					</section>

					<section>
						<h2>So what is IoT?</h2>
							<ul>
		         				<li>The Internet of Things!</li>
		         				<li>Numerous protocols for communication</li>
		         				<li>Everything is connected, from RFID to Bluetooth to WiFi and everything in between</em></li>
	          					<li>How can we leverage this in new applications?
		          					<ul>
										<li>We want to take advantage of all this available data</li>
									</ul>
								</li>
	          				</ul>
	          			<aside class="notes">
	          				<ul>
	          					
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Many Devices</h2>
						<ul>
							<li>Tens of Billions and counting</li>
	         				<li>Devices often very “thin"
	         					<ul>
									<li>Lack processing power, memory and battery life</li>
								</ul>
							</li>
	         				<li>Bandwidth Concerns
	         					<ul>
									<li>Data ain't cheap!!</li>
								</ul>
							</li>
	         				<li>Need to process as much as possible on the server</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Easier to manage if more processing is on the server</li>
	          					<li>Sometimes we HAVE to process things on the device</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>What's so difficult?</h2>
						<ul>        				
	         				<li>Scaling out
	         					<ul>
									<li>Much more difficult than scaling up</li>
									<li>This requires a different architecture than a typical web application</li>
	          						<li>Many different devices, different data streams, different information</li>
	          						<li>We don't want a different core application per device type</li>
								</ul>
							</li>
							<li>What to do with all this data?!?
		         					<ul>
										<li>Big data needs big processing!</li>
									</ul>
								</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>scaling out rather than up</li>
	          					<li>Privacy and Security concerns</li>
	          					<li>Complexities of interoperability of devices
									<ul>
										<li>Protocols, Interfaces, Algorithms and Discovery</li>
										<li>Standards are (very slowly) emerging</li>
									</ul>
		         				</li>
		         				<li>Intermittent connectivity</li>		         				
	          				</ul>
	          			</aside>
					</section>					

					<section>
						<h2>Many Emerging Architectures and Stacks</h2>
						<ul>
							<li>Lambda</li>
							<li>Kappa</li>
							<li>SMACK</li>
							<li>Zeta</li>
							<li>IoT-a</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Greek letters are all the rage</li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

				</section>

				<section>

					<section>
						<h2>Architectures</h2>
						<h4>So what does this look like?</h4>
						<img src="images/igotthis.jpg"/>
						</p>
					</section>

					<section>
						<h2>First... CAP Theorem</h2>
						<h4>So what does this look like?</h4>
						<img src="images/cap-theorum.png"/>
						</p>

						<aside class="notes">
	          				<ul>
	          					<li>Basically a choice between Consistency and Partition tolerance
	          						<ul>
	          							<li>If you want scalability</li>
	          						</ul>
   								</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>First... CAP Theorem (cont...)</h2>
						<ul>
			         		<li><strong>Consistency</strong>
			         			<ul>
			         				<li>Across different partitions/nodes. Every client has the same view.</li>
			         				<li>Immediate not Eventual</li>
		         				</ul>
		         			</li>
		         			<li><strong>Availability</strong>
			         			<ul>
			         				<li>The system is always available, responses are guaranteed.</li>
			         				<li>Your queries are always executed.</li>
		         				</ul>
		         			</li>
		         			<li><strong>Partition Tolerance</strong>
			         			<ul>
			         				<li>No single point of failure</li>
			         				<li>Can handle a node disappearing</li>
			         				<li>Replication across nodes</li>
		         				</ul>
		         			</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Can only guarantee two of the three database properties</li>
	          					<li>Basically a choice between Consistency and Partition tolerance
	          						<ul>
	          							<li>If you want scalability</li>
	          						</ul>
   								</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Lambda Architecture</h2>
						<img class="stretch" src="images/lambda-conceptual.png"/>

						<aside class="notes">
	          				<ul>
	          					<li>An immutable sequence of records is captured and fed in</li>
	          					<li>Queries are executed upon a combination of the realtime and batch layers</li>
	          					<li>Events are persisted to be replayed, either using kafka on input or to a historical database</li>
	          				</ul>
	          			</aside>
					</section>


					<section>
						<h2>Lambda Architecture (cont...)</h2>
						<ul>
			         		<li>Originally described by Nathan Marz, creator of Apache Storm</li>
		         			<li>Claims to beat CAP theorem</li>
		         			<li>Splits the application into three parts
			         			<ul>
			         				<li><strong>Batch</strong> and <strong>Realtime</strong> layers for ingesting data</li>
			         				<li><strong>Serving</strong> layer for querying data</li>
		         				</ul>
		         			</li>
		         			<li>Events (incoming data) are time based
			         			<ul>
			         				<li><strong>Immutable, append only</strong></li>
			         				<li>Series of <em>Commands</em></li>
			         				<li>No updates or deletes</li>
			         				<li>Can replay the data</li>
		         				</ul>
		         			</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Duplication between layers</li>
	          					<li>Queries are executed upon a combination of the realtime and batch layers</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Lambda Architecture - Realtime Layer</h2>
						<ul>
			         		<li>Realtime layer handles new data</li>
			         		<li>As accurate as possible, however the batch layer gives “the final answer"</li>
			         		<li>CAP complexity is isolated into this layer, which now only applies to the latest subset of data</li>
			         		<li>No need to worry about corrupting the data</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Realtime data not yet incorporated into the batch layer</li>
	          					<li>Data replayed in batch layer</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Lambda Architecture - Batch Layer</h2>
						<ul>
			         		<li>Batch layer handles historical data</li>
			         		<li>Queries in the realtime layer are recomputed after the data is persisted to the batch layer</li>
			         		<li>Allows issues in the realtime layer to be corrected</li>
			         		<li>More complex and involved computations, not time critical</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Replaying data after fixing bugs etc</li>
	          					<li>Data replayed in batch layer</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Lambda Architecture - Serving Layer</h2>
						<ul>
			         		<li>The two “views” are merged together to give a final answer by the serving layer
			         			<ul>
			         				<li>Batch layer is indexed for querying</li>
		         				</ul>
		         			</li>
			         		<li>Many different options here depending on query use cases</li>
			         		<li>Table/View per query?</li>
	          			</ul>
	          			<aside class="notes">
	          				<ul>
	          					<li>There's no standard way of implementing this layer</li>
	          					<li>Cassandra, HBase, Apache Drill, VoltDB</li>
	          					<li>We're just presenting aggregated views for fast querying</li>
	          					<li>Could use Akka persistence and hold the latest state in memory</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<img class="stretch" src="images/lambda-logical.png"/>
					</section>


					<section>
						<h2>Lambda Architecture - The good stuff</h2>
						<ul>
			         		<li>Fault Tolerant against both hardware and humans</li>
			         		<li>Stores all input data unchanged, allowing for reprocessing
			         			<ul>
			         				<li>We do this in our Muvr application to re-categorise exercises with new models</li>
		         				</ul>
			         		</li>
			         		<li>Allows for a realtime view of the data, plus a more accurate higher latency view
			         		</li>
	          			</ul>
	          			<aside class="notes">
	          				<ul>
	          					<li>Clean separation of concerns</li>
	          					<li>If done correctly gives fast reads and fast writes</li>
	          					<li>Scalable</li>
	          				</ul>
	          			</aside>
					</section>


					<section>
						<h2>Lambda Architecture - The bad stuff</h2>
						<ul>
			         		<li>Assumes accurate results can’t be computed real-time</li>
			         		<li>Duplicates the transformation and computation logic (code) in both layers</li>
			         		<li>More complex to debug as a result of the two discrete implementations</li>
			         		<li>Data still not immediately consistent, hence doesn’t beat CAP theorem</li>
	          			</ul>
	          			<aside class="notes">
	          				<ul>
	          					<li>Duplication is a BIG issue here</li>
	          					<li>Frameworks such as summingbird attempt to address this but aren't without their issues and complexities</li>
	          					<li>Can be complex creating the required combined views</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Kappa Architecture</h2>
						<img class="stretch" src="images/kappa-conceptual.png"/>
					</section>

					<section>
						<h2>Kappa Architecture (cont...)</h2>
						<ul>
			         		<li>Coined by Jay Creps, formally of LinkedIn</li>
		         			<li>Simplifies the Lambda architecture by removing the batch layer</li>
		         			<li>Streams are capable of persisting the historical data
			         			<ul>
			         				<li>For example Kafka and Event Sourcing</li>
		         				</ul>
		         			</li>
		         			<li>The assumption is that stream processing is powerful enough to process and transform the data real-time</li>
		         			<li>Processed data can then be stored in a table for querying</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Sometimes we can't process the data real time</li>
	          					<li>We can still archive data for analytics jobs</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Kappa Architecture - Reprocessing</h2>
						<ol>
			         		<li>Start a second instance of the stream processor</li>
		         			<li>Replay the data from the desired point in time</li>
		         			<li>Output to a new table</li>
		         			<li>Once complete, stop the original stream processor</li>
		         			<li>Processed data can then be stored in a table for querying
		         				<ul>
	          						<li>Delete the old table</li>
		          				</ul>
		          			</li>
	          			</ol>

	          			<aside class="notes">
	          				<ul>
	          					<li>Can be done in a live application</li>
	          					<li>The original stream is the unaltered data</li>
	          					<li>Can always be relied upon as it's unaltered</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<img class="stretch" src="images/kappa-logical.png"/>
					</section>

					<section>
						<h2>Kappa Architecture - The good stuff</h2>
						<ul>
			         		<li>Fault Tolerant against both hardware and humans</li>
			         		<li>No need to maintain two separate code bases for the layers</li>
			         		<li>Data only reprocessed when stream processing code has changed
			         			<ul>
	          						<li>Potentially reduced hosting costs</li>
		          				</ul>
			         		</li>
			         		<li>Many of the same advantages as the Lambda architecture</li>
	          			</ul>
	          			<aside class="notes">
	          				<ul>
	          					<li>Clean separation of concerns</li>
	          					<li>Still need to create aggregated views for fast querying</li>
	          					<li>Depends on use case of course	          						
		          					<ul>
		          						<li>Posting the results vs making them available for querying</li>
			          				</ul>
			          			</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Kappa Architecture - The bad stuff</h2>
						<ul>
			         		<li>May be difficult / impossible to perform the computation on a large enough dataset
			         			<ul>
		          					<li>Stream window or stateful datasource / cache</li>
		          					<li>Could introduce latency</li>
			          			</ul>
			         		</li>
			         		<li>Often you’ll need to decide on a small “window” of historical data</li>
			         		<li>Might not be sufficient for complex Machine Learning</li>
	          			</ul>
	          			<aside class="notes">
	          				<ul>
	          					<li>Would still need to train models for machine learning, which means batch layer is escence</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Quick summary</h2>
						<ul>
			         		<li>Streams eveywhere!!</li>
			         		<li>Separating history from current (aggregated) data
			         			<ul>
		          					<li>Denormalised output data</li>
		          					<li>Input format doesn’t need to match output format</li>
		          					<li>Immutability means Non-destructive writes</li>
			          			</ul>
			         		</li>
			         		<li>Not new ideas, just a different way of thinking about them</li>
			         		<li>Enforce a company wide data format for messages early on</li>
	          			</ul>
	          			<aside class="notes">
	          				<ul>
	          					<li>High throughput important</li>
	          					<li>Some ideas of ESBs apply here, better decoupling</li>
	          					<li>AVRO (Schemas)</li>
	          					<li>Schema ensures data correctness, improves clarity and understanding, improves compatibility</li>
	          					<li>Loose coupling</li>
	          					<li>Dealing with Schema evolution</li>
	          					<li>Dealing with corrupt Schemas</li>
	          					<li>Can use DB in combination with journal for historical data</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Muvr use case</h2>
						<ul>
							Originally used classic Lambda
			         		<li>Akka cluster for speed layer</li>
			         			<ul>
			         				<li>One Actor per user</li>
			         				<li>CQRS and (Akka) Event Sourcing</li>
		         				</ul>
		         			<li>Spark (ML) batch layer</li>
			         			<ul>
			         				<li>Trains the classifiers</li>
		         				</ul>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Explain what Muvr is!</li>
	          					<li>Great in theory!</li>
	          				</ul>
	          			</aside>
					</section>	

					<section>
						<img class="stretch" src="images/muvr-lambda-wrong.png"/>
					</section>		

					<section>
						<h2>Muvr use case - revised</h2>
						<ul>
							<span>Simply no good in production!</span>
			         		<li>Latency too high</li>
			         			<ul>
			         				<li>Too sensitive to network</li>
			         				<li>Required always on connectivity</li>
		         				</ul>
		         			<li>We had to move the speed layer onto the device</li>
			         			<ul>
			         				<li>Or rather split it between server and device</li>
		         				</ul>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>It's designed to be used in a gym!!</li>
	          				</ul>
	          			</aside>
					</section>	

					<section>
						<img class="stretch" src="images/muvr-lambda-updated.png"/>
					</section>	

					<section>
						<h2>S.M.A.C.K</h2>
			
	          			<img class="stretch" src="images/smack.png"/>

	          			<aside class="notes">
	          				<ul>
	          					<li>These technologies are being used together more and more</li>
	          				</ul>
	          			</aside>
					</section>		

					<section>
						<h2>S.M.A.C.K</h2>
						<ul>
			         		<li><strong>S</strong>park</li>
			         			<ul>
			         				<li>Fast, distributed data processing and analytics engine</li>
		         				</ul>
		         			</li>
		         			<li><strong>M</strong>esos</li>
			         			<ul>
			         				<li>Cluster resource management solution providing resource isolation and sharing across distributed systems</li>
		         				</ul>
		         			</li>
		         			<li><strong>A</strong>kka</li>
			         			<ul>
			         				<li>Actor based JVM framework for building highly scalable, concurrent distributed message-driven applications</li>
		         				</ul>
		         			</li>
		         			<li><strong>C</strong>assandra</li>
			         			<ul>
			         				<li>Highly available, distributed, scalable database designed to handle huge amounts of data across multiple data centers</li>
		         				</ul>
		         			</li>
		         			<li><strong>K</strong>afka</li>
			         			<ul>
			         				<li>A high-throughput, low-latency distributed stream based messaging system designed to handle and persist events</li>
		         				</ul>
		         			</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li></li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>
				
				</section>

				<section>
					<section>
						<img class="stretch" src="images/spark.png"/>
						<aside class="notes">
							<ul>
		          				<li>Key to all these fancy architectures</li>
		          				<li>becoming the defacto data processing engine</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Apache Spark</h2>

						<span>Apache Spark™ is a fast and general purpose engine for large-scale data processing, with built-in modules for streaming, SQL, machine learning and graph processing</span>

						<ul>
	          				<li>Originally developed by Berkeley’s AMP Lab in 2009</li>
	          				<li>Open sourced as part of Berkeley Data Analytics Stack (BDAS) in 2010</li>
	          				<li>Top level apache project since 2014</li>
	          			</ul>
	          			<aside class="notes">
							<ul>
		          				<li>Spark provides rich APIs in Java, Scala, Python, and R</li>
		          				<li>The fundamental programming abstraction in Spark is the Resilient Distributed Dataset (RDD)</li>
		          				<li>can be have crazy performance improvements over vanilla MapReduce</li>
		          				<li>Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Who uses Spark</h2>

						<img class="stretch" src="images/powered-by-spark.png"/>
					</section>

					<section>
						<h2>Why Spark</h2>

						<img src="images/spark-github.png"/>

						<ul>
	          				<li>Contributers from over 200 companies</li>
	          				<li>One of the most active open source projects</li>
	          				<li>IBM will invest roughly $300 million over the next few years and assign 3500 people to help develop Spark</li>
	          			</ul>
	          			<aside class="notes">
							<ul>
		          				<li></li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Why Spark cont...</h2>
						<ul>
	          				<li>Easy to develop
	          					<ul>
			         				<li>Flexible, composable programming model</li>
			         				<li>Provides Spark Shell</li>
			         				<li>APIs for Scala, Java and Python</li>
		         				</ul>
	          				</li>
	          				<li>Fast and Scalable!
	          					<ul>
			         				<li>Optimised storage between memory and disk</li>
			         				<li>Scales from a single laptop to a large cluster</li>
			         				<li>Up to 10x-100x faster than Hadoop</li>
		         				</ul>
	          				</li>
	          				<li>Feature Rich
	          					<ul>
			         				<li>Supports Event Streaming Applications</li>
			         				<li>Efficient support for Machine Learning</li>
			         				<li>Modular architecture</li>
		         				</ul>
	          				</li>
	          			</ul>
	          			<aside class="notes">
							<ul>
								<li>Lazily evaluates this so that...</li>
		          				<li>Spark can work out the most efficient way to do things</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Spark usage scenarios</h2>
						<ul>
	          				<li>On demand querying of large datasets</li>
	          				<li>Batch processing</li>
	          				<li>Machine Learning</li>
	          				<li>Stream Processing</li>
	          				<li>Data Transformations</li>
	          				<li>Graph processing</li>
	          			</ul>
	          			<aside class="notes">
							<ul>
		          				<li>Modular architecture makes it easy to extend functionality</li>
		          				<li>Spark needs data from somewhere, it's not a database</li>
		          				<li>So going back to previous slides, spark can run as the batch layer, running analytics which can them be output to tables (lambda)</li>
		          				<li>Or we can run a Spark Streaming job which can perform smaller calculations on the fly (Kappa) - on the fly</li>
		          				<li>Could actually re-use a lot of the code between layers</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Interactive Shells</h2>
						
						<ul>
							<li><code>spark-shell</code>
	          					<ul>
			         				<li>Extended Scala REPL with Spark imports already in scope</li>
		         				</ul>
	          				</li>
	          				<li><code>spark-submit</code>
	          					<ul>
			         				<li>Used to submit jobs (JARs) to the cluster</li>
		         				</ul>
	          				</li>
	          				<li><code>spark-sql</code>
	          					<ul>
			         				<li>SQL REPL</li>
		         				</ul>
	          				</li>
	          				<li><code>pyspark</code>
	          					<ul>
			         				<li>Python shell</li>
		         				</ul>
	          				</li>
	          			</ul>
	          			<aside class="notes">
							<ul>
		          				<li>Shells make programming so much easier</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<img class="stretch" src="images/spark-shell.png"/>
					</section>

					<section>
						<h2>Spark Stack</h2>

						<img class="stretch" src="images/spark-stack.png"/>

						<aside class="notes">
							<ul>
		          				<li>Architecture allows new modules to be added</li>
		          				<li>Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built on top of. It provides in-memory computing capabilities to deliver speed, a generalized execution model to support a wide variety of applications, and Java, Scala, and Python APIs for ease of development.</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Spark Jobs</h2>

						<img class="stretch" src="images/spark-job-scheduling.png"/>

						<aside class="notes">
							Directed Acyclic Graph
							<ul>								
		          				<li>The driver breaks the RDD graph down into jobs</li>
		          				<li>Each job is composed of stages</li>
		          				<li>Stages get implemented by one or more tasks</li>
		          				<li>Tasks are sent to the executors to perform the work</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Spark Cluster Overview</h2>

						<img class="stretch" src="images/spark-cluster-abstraction.png"/>

						<aside class="notes">
							<ol>							
								<li>Driver connects to manager. Asks it for resources</li>
								<li>Manager assigns nodes, allocates resources</li>
								<li>Executor started on each assigned node, isolated from other applications</li>
								<li>Tasks sent to executors and started on nodes - multi threaded</li>
							</ol>
							<ul>
								<li>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.</li>
		          				<li>Executor lives for the length of the whole application (1-1).</li>
		          				<li>A Job consistens of multiple tasks spawned in response to a Spark action</li>		          			     				
		          				<li>The cluster manager manages all these workers, yarn or mesos could do this for you</li>
		          				<li>The driver is where your code lives, you can submit a packaged application or use the REPL</li>
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>RDDs</h2>
						<ul>
							<li>Immutable
	          					<ul>
			         				<li>Each transformation will create a new RDD</li>
		         				</ul>
	          				</li>
	          				<li>Lazy
	          					<ul>
			         				<li>A DAG (directed acyclic graph) of computation is constructed</li>
			         				<li>The actual data is processed only when an action is invoked</li>
		         				</ul>
	          				</li>
	          				<li>Reusable
	          					<ul>
			         				<li>Can re-use RDDs by executing them multiple times</li>
		         				</ul>
	          				</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>An RDD is an immutable distributed collection of objects that can be operated on in parallel</li>
								<li>RDDs are the workhorse of Spark, everything is done using RDDs.</li>
								<li>Everything is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result</li>
		          				<li>Users create RDDs in three ways: from an RDD, by loading an external dataset, or by distributing a collection of objects (e.g., a list or set) in their driver program, not done in production as the whole dataset must be in memory</li>	
		          				<li>Persistent in memory between operations</li>	     
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>RDD Partitions</h2>
						<img class="stretch" src="images/rdd-partitions.png"/>


						<aside class="notes">
							<ul>
								<li>RDD is divided into partitions which are districuted across the cluster</li>	
								<li>Partitions never span multiple machines, i.e. tuples in the same partition are guaranteed to be on the same machine.</li>	 
								<li>The number of partitions to use is configurable. By default, it equals the total number of cores on all executor nodes.</li>         
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>RDD Tasks</h2>
						<img class="stretch" src="images/rdd-tasks.png"/>


						<aside class="notes">
							<ul>
								<li>One task per partition (on a per stage basis)</li> 
		          				<li>task is smallest unit of computation</li>       
		          				<li>partition is smallest unit of data</li>      
		          			</ul>
	          			</aside>
					</section>


					<section>
						<h2>RDD Operations</h2>
						RDD operations are split into two distinct categories
						<p/>
						<ul>
							<li><strong>Transformations</strong>
	          					<ul>
			         				<li>Returns a new RDD which will apply the transformation</li>
			         				<li>Can merge multiple RDDs into a new RDD (<code>union</code>)</li>
			         				<li><code>map, filter, flatMap, mapPartitions, join</code></li>
		         				</ul>
	          				</li>
	          				<li><strong>Actions</strong>
	          					<ul>
	          						<li>Force evaluation of the transformations</li>
			         				<li>Return a final <strong>value</strong> to the driver program or write data to an external storage system. </li>		         	
			         				<li><code>reduce, collect, count, saveAs**, foreach</code></li>
		         				</ul>
	          				</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>No substantial benefit to writing a single complex map instead of chaining together many simple operations.</li>
								<li>Each operation can be executed on a different node</li>
								<li>There are different types of RDDs, for example Numeric RDDS and PairRDDS... Each has some additional operations available. mean for example</li>
								<li>PairRDDs are really powerful and can be used to control the partitioning of data by key, reducing shuffling of data. Each key can then executed in parallel across nodes</li>
								<li>Using PairRDDs lets you perform operations just on values, much more efficient as the data will be on the same partition</li>
								     
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>RDD Lineage</h2>
						<img class="stretch" src="images/rdd-lineage.png"/>
						<p/>
						<ul>
							<li>Each operation on an RDD creates a new RDD, with the previous operation as part of it's history.</li>
	          				<li>A lost RDD partition is reconstructed from ancestors</li>
	          				<li>By default the whole RDD lineage is executed when an action is invoked
	          					<ul>
	          						<li>We can avoid this by caching (persisting) at various stages</li>
	          					</ul>
	          				</li>
						</ul>

						<aside class="notes">
							<ul>
								<li>RDDs know their parents, which also know their parents...</li>
								<li>As you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. It uses this information to compute each RDD on demand and to recover lost data if part of a persistent RDD is lost.</li>	        
		          			</ul>
	          			</aside>
					</section>

					<section>
						<h2>Basic Word Count</h2>

						<span>First add the imports and create the <code>SparkContext</code></span>

						<pre><code data-trim class="scala">
import org.apache.spark.{SparkConf, SparkContext}

//Create a conf, running on two local cores
val conf  = new SparkConf()
   .setAppName("Simple Application")
   //* will use all cores
   .setMaster("local[2]")

val sc = new SparkContext(conf)
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li></li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Basic Word Count (cont...)</h2>

						<span>We can now create an RDD from our <code>SparkContext</code>, we can then transform the RDD and execute it</span>

						<pre><code data-trim class="scala">
val input = sc.textFile("words.txt")
// Split up into words.
val words = input.flatMap(line => line.split(" "))

val counts = words
   //pair each word with 1
   .map(word => (word, 1))
   //combine all matching words
   .reduceByKey{case (x, y) => x + y}

// Save the word count back out to a text file.
counts.saveAsTextFile("counts.txt")
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li></li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Basic Word Count (cont...)</h2>

						<span>The result is a file containing the following</span>

						<pre><code data-trim class="scala">
...
(daughter,1)
(means,1)
(this,2)
(brother,1)
(possession,1)
(is,1)
(term,1)
...
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li></li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Debugging Word Count</h2>

						<span><code>toDebugString</code> lets us print out the RDD lineage for debugging</span>

						<pre><code data-trim class="scala">
println(counts.toDebugString)

(2) ShuffledRDD[4] at reduceByKey at BasicWordCount.scala:25 []
 +-(2) MapPartitionsRDD[3] at map at BasicWordCount.scala:24 []
    |  MapPartitionsRDD[2] at flatMap at BasicWordCount.scala:19 []
    |  MapPartitionsRDD[1] at textFile at BasicWordCount.scala:14 []
    |  words.txt HadoopRDD[0] at textFile at BasicWordCount.scala:14 []
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li></li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Monitoring</h2>
						
						<ul>
							<li><code>http://localhost:4040</code>
	          					<ul>
			         				<li>Spark’s “stages” job console. Started by the SparkContext.</li>
		         				</ul>
	          				</li>
	          				<li><code>http://master_host_name:4040</code>
	          					<ul>
			         				<li>For Spark Standalone clusters, the Spark Master.</li>
		         				</ul>
	          				</li>
	          				<li><code>http://slave_host_name:7077</code>
	          					<ul>
			         				<li>For Spark slave nodes</li>
		         				</ul>
	          				</li>
	          			</ul>
	          			<aside class="notes">
							<ul>
		          				<li>Shells make programming so much easier</li>
		          			</ul>
	          			</aside>
					</section>

					<section>	
						We can view all jobs since the <code>SparkContext</code> was started		
						<img class="stretch" src="images/spark-console-jobs.png"/>
					</section>

					<section>	
						Clicking on a job displays more fine grained details
						<img class="stretch" src="images/spark-console-details.png"/>
					</section>

					<section>
						The lineage graph is really useful for debugging						
						<img class="stretch" src="images/spark-console-dag.png"/>
					</section>

				</section>

				<section>

					<section>
						<h2>Spark Streaming Intro</h2>

						<img class="stretch" src="images/streaming-arch.png"/>
						<ul>
			         		<li>High-throughput streaming from live events</li>
		         			<li>Run event based computations and update data in real-time</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Data can be ingested by RECEIVERS from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window.</li>
	          					<li>Can have reliable and unreliable receivers</li>
	          					<li>Can store raw data or aggregated (processed) data... or both!</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Spark Streaming Intro (cont...)</h2>

						<img class="stretch" src="images/streaming-flow.png"/>
						<ul>
			         		<li>Data is grouped into batch windows for processing</li>
		         			<li>The &quot;batch interval&quot; is configurable</li>
		         			<li>Spark provides a high level abstraction called a <em>discretized stream</em> or <code>DStream</code></li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Spark receives live input data streams and divides the data into micro batches, which are then processed by the Spark engine to generate the final stream of results in batches</li>
	          					<li>Micro-batches of data are created at regular time intervals.</li>
	          					<li>This stream of micro batches is known as a DStream</li>
	          					<li>DStreams created from input streams or other DStreams</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>DStreams</h2>

						<img class="stretch" src="images/streaming-flow-rdd.png"/>
						<ul>
			         		<li>A DStream is a continuous sequence of RDDs
			         			<ul>
					         		<li>Each micro batch of data is an RDD</li>
			          			</ul>
			          		</li>
		         			<li>Each RDD has lineage and fault tolerance</li>
		         			<li>Transformations similar to those on normal RDDs are applicable</li>
		         			<li>There are many additional transformations and output operations that are only applicable to discretized streams</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Any operation applied on a DStream translates to operations on the underlying RDDs</li>
	          					<li>Batch interval or time step is used to configure this, typically between 500ms and several seconds</li>
	          					<li>Transformations are called on a DStream and applied to each RDD individually</li>
	          					<li>Both stateless and stateful transformations</li>
	          					<li>Stateful transformaitons can be used to keep counts or process aggregations on the fly</li>
	          					<li>Although aggregating in something like Cassandra is recommended instead</li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Basic Stream Printer</h2>

						<span>First let add the imports and create a <code>StreamingContext</code></span>

						<pre><code data-trim class="scala">
import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming._

val conf = new SparkConf()
   .setAppName("Streaming Word Count")
   .setMaster("local[2]")

val ssc = new StreamingContext(conf, Seconds(1))
val context = ssc.sparkContext
	          			</code></pre>
	          			<aside class="notes">
	          				We must have more cores available than receivers.
	          				<ol>
	          					<li>Define the input sources by creating input DStreams.</li>
								<li>Define operations to be applied to the batches</li>
								<li>Start receiving data and processing it using streamingContext.start()</li>
								<li>Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination()</li>
								<li>The processing can be manually stopped using streamingContext.stop()</li>
	          				</ol>
	          			</aside>
					</section>

					<section>
						<h2>Basic Stream Printer (cont...)</h2>

						<span>Next we need to define the receiver, here we're using a queue</span>

						<pre><code data-trim class="scala">
//We're using a mutable queue with 3 items
val rddQueue: Queue[RDD[String]] = Queue()
rddQueue += context.textFile("words.txt")
rddQueue += context.textFile("words2.txt")
rddQueue += context.textFile("words3.txt")
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>The queue has just 3 items, we could replace this with a socketTextStream, kafka, twitter, etc. But this makes it easy to run the examples</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Basic Stream Printer (cont...)</h2>

						<span>Finally we can read, parse and start the stream</span>

						<pre><code data-trim class="scala">
val streamBatches = 
   ssc.queueStream(rddQueue, oneAtATime = true)

val words = streamBatches.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

//print the first 10 elements of each batch
wordCounts.print()

//Start the stream and wait for it to terminate
ssc.start()
ssc.awaitTermination()
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>the queue acts as the source receiver, queueStream removes one item each time, so 3 batches in total</li>
	          					<li>Once a context has been started, no new streaming computations can be set up or added to it.</li>
								<li>Once a context has been stopped, it cannot be restarted.</li>
								<li>Only one StreamingContext can be active in a JVM at the same time.</li>
	          					<li>flatMap will split each line into multiple words and the stream of words is represented as the words DStream</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Basic Stream Printer output</h2>
						<pre><code data-trim class="scala">
-------------------------------------------
Time: 1452128900000 ms
-------------------------------------------
(daughter,1)
(brother,4)
...

-------------------------------------------
Time: 1452128901000 ms
-------------------------------------------
(too,,1)
(Gauls,3)
...
...
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>The output of the current batch is printed each time</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Stateful Stream Printer</h2>

						<span>Lets extend this example by persisting the state between batches</span>

						<pre><code data-trim class="scala">
val ssc = new StreamingContext(conf, Seconds(1))
// We need somewhere to store the state
// So we create a checkout directory
ssc.checkpoint("./checkpoint") 
//Create a stream using a Dummy ReceiverInputDStream
val stream = ssc.receiverStream(
   new DummySource(ratePerSec = 1))

// Parse the stream words and pair them up
val wordDStream = 
   stream.flatMap(_.split(" ")).map(x => (x, 1))
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>The DummySource is a custom receiver which returns the same set of words each time, in this case once per second</li>
	          					<li>Notice we're not reducing them here</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Stateful Stream Printer (cont...)</h2>

						<span>Lets extend this example by persisting the state between batches</span>

						<pre><code data-trim class="scala">
def stateMappingFunc(batchTime: Time, key: String,
    value: Option[Int], state: State[Long]
    ): Option[(String, Long)] = {
  val currentVal = value.getOrElse(0).toLong
  val aggVal = state.getOption.getOrElse(0L)
  val sum = currentVal + aggVal
  val output = (key, sum)
  state.update(sum)
  Some(output)
}
val stateSpec = StateSpec
  .function(stateMappingFunc _)
  .numPartitions(2)
  .timeout(Seconds(60))
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
      							<li>For mapWithState need to define a function which can combine the state of two DStreams</li>
      							<li>This is used to update the state of the previous batch with that of the new one</li>
      							<li> The function must accept the following params:</li>
     						 	<li> The current Batch Time</li>
      							<li> The key for which the state needs to be updated</li>
      							<li> The value observed at the 'Batch Time' for the key.</li>
      							<li> The current state for the key.</li>
      							<li> Return: the new (key, value) pair where value has the updated state information
	          					<li>The state Spec wraps the function and allows us to define properties such as initialState and partitions</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Stateful Stream Printer (cont...)</h2>

						<span>Finally merge the states using the function and start the stream</span>

						<pre><code data-trim class="scala">
// apply the function returning a merged stream
val wordCountStateStream = 
   wordDStream.mapWithState(stateSpec)

//Print the first 10 records of the stream
wordCountStateStream.print()

ssc.start()
ssc.awaitTermination()
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
      							<li>Since we merge the streams based on keys, this stream will contain the same # of records as the input dstream</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Stateful Stream Printer Output</h2>
						<pre><code data-trim class="scala">
Time: 1452011283000 ms
-------------------------------------------
(buy,2)
(as,7)
(as,8)
(burden,2)
....
-------------------------------------------
Time: 1452011284000 ms
-------------------------------------------
(buy,3)
(as,12)
(as,13)
(burden,3)
....
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
      							<li>This time the output is cumulative, incrementing words between batches</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Windowed DStreams</h2>

						<img class="stretch" src="images/streaming-windowed.png"/>
						<ul>
			         		<li>Can compute results based on data in multiple batches</li>
		         			<li>Known as window(ed) transformations</li>
		         			<li>Carried over multiple batches in a sliding window</li>
		         			<li>Specify the <em>window length</em> and <em>sliding interval</em></li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li><em>window length</em> - The duration of the window (3 in the figure).</li>
								<li><em>sliding interval</em> - The interval at which it's performed (2 in the figure).</li>
	          					<li>Requires a checkpoint directory for stateful transformations</li>
	          					<li>For fault tolerance</li>
	          					<li>The configuration allows you to define the overlap for stateful transformations</li>
	          					<li>Every time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream. 
	          					</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Twitter Hashtag Counter</h2>

						<span>This time we're consuming from the live Twitter API</span>

						<pre><code data-trim class="scala">
//Receive all tweets for the scala hashtag filters
val stream = TwitterUtils.createStream(ssc, None)

//Split tweets into words
val words = stream.flatMap(tweet => 
   tweet.getText.toLowerCase.split(" "))

//Pull out the hashtags
val hashtags = words
   .filter(word => word.startsWith("#"))
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>Each item in the stream is an object, containing various Tweet properties, time, user, retweet, etc</li>
	          					<li>You can pass filters to the createStream</li>
	          					<li>We're not doing anything fancy with re-tweets etc</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Twitter Hashtag Counter</h2>

						<span>We want to count all tweets in the last 60 seconds</span>

						<pre><code data-trim class="scala">
val tagCountsOverWindow = hashtags
   .map((_, 1))
   //60 second window
   .reduceByKeyAndWindow(_ + _, Seconds(60))

val sortedTagCounts = tagCountsOverWindow
   //Swap the key and value to make sorting easier
   .map { case (hashtag, count) => (count, hashtag) }
   .transform(_.sortByKey(false))

//Print the Top 10
sortedTagCounts.print()
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>This requires shuffling data as we're swapping the keys around</li>
	          					<li>transform lets us apply RDD specific functions that DStreams don't expose</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Twitter Hashtag Counter Output</h2>
						<pre><code data-trim class="scala">
-------------------------------------------
Time: 1452124409000 ms
-------------------------------------------
(7,#eurekamag)
(7,#wastehistime2016)
(6,#aldub25thweeksary)
(6,#videomtv2015)
...
-------------------------------------------
Time: 1452124411000 ms
-------------------------------------------
(8,#wastehistime2016)
(7,#eurekamag)
(6,#aldub25thweeksary)
(6,#videomtv2015)
...

	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>


				</section>


				<section>

					<section>
						<h2>Spark on Cassandra</h2>
						<img class="stretch" src="images/spark-on-cassandra.png"/>
	          			<aside class="notes">
	          				<ul>
	          					<li>Specifically on DSE which makes deployment much easier</li>
	          					<li>A Spark worker is spawned which is responsible for the executors. </li>
	          					<li>This has it’s own memory allocation which isolates it from Cassandra</li>
	          					<li>Each worker node sits on an analytics only Cassandra DSE instance with an analytics data centre</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Deployment on Cassandra</h2>
						<img class="stretch" src="images/cassandra-spark-dc.png"/>
	          			<aside class="notes">
	          				<ul>
	          					<li>Spark specific data center</li>
	          					<li>Spark worker living on each Cassandra node</li>
	          					<li>Prevents Spark interferring with norml queries</li>
	          					<li>Data is replicated by Cassandra between the data centres, both read+write</li>
	          					<li>Configured using the dse.yaml</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Spark Cassandra Connector</h2>
						<ul>
							<li>Lets you expose Cassandra tables as Spark RDDs, write Spark RDDs to Cassandra tables, and execute CQL queries in your Spark applications</li>
			         		<li>Open sourced by Datastax, not DSE specific</li>
		         			<li>Spark partitions are constructed from data stored by Cassandra on the same node</li>
		         			<li>Partitions are not computed until an action is seen</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Shuffling is expensive, so it's important to get partitioning correct</li>
	          					<li>Some operations explicitly shuffle data</li>
	          					<li>Allows complex queries within a partition, although Cassandra getting more and more built in functionality to do this</li>
	          					<li>Can do table wide scans, thought slower than HDFS</li>
	          					<li>Can use Spark to create agg tables, effectively materialised views</li>
	          					<li>Cassandra 3 has materialised views</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Cassandra Aware Partitioning</h2>
						<img class="stretch" src="images/spark-cassandra-partitioning.png"/>

	          			<aside class="notes">
	          				The Cassandra connector uses Cassandra partition information to find and process data on local nodes
	          				<ol>
	          					<li>Nodes in the Cassandra Cluster own part of the token range</li>
	          					<li>Spark connector keeps track of where all these live</li>
	          					<li>These ranges are then mapped to Spark partitions</li>
	          					<li>The connector uses this information to select Spark nodes local to the Cassandra data</li>
	          				</ol>
	          				<ul>
	          					<li>We can repartition datasets to better suit our application and data</li>
	          					<li>The rdd is on the same node as the node who hold that cassandra partition key</li>
	          					
	          				</ul>
	          			</aside>
					</section>


					<section>
						<h2>Cassandra Aware Partitioning (cont...)</h2>
						<ul>
			         		<li>Nodes in the Cassandra Cluster own part of the token range</li>
			         		<li>Spark connector keeps track of where all these live</li>
			         		<li>These ranges are then mapped to Spark partitions</li>
			         		<li>So each Spark partition knows which C* node it's data is on</li>
			         		<li>The Spark driver &amp; executor can therefore assign the task to the appropriate node</li>
			         		<li>Processing local data is faster!!</li>
	          			</ul>

	          			<aside class="notes">
	          				<ul>
	          					<li>Lets ignore vnodes for simplicity</li>
	          					<li>Uses spark.cassandra.split.size  to determine how to break up the ranges into these pieces</li>
          						<li>This is an estimate as it doesn’t read all the data</li>
     							<li>As in it knows which Cassandra node the data is on</li>
     							<li>Depends on the RDD being partition in a similar way to the Cassandra table</li>
     							<li>spark.locality.wait property defines how long to wait for the preferred partition/node</li>
     							<li>data is paged out according to the spark.cassandra.input.page.row.size property</li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Cassandra Stream Word Count</h2>

						<span>First we need the appropriate imports and create the context</span>

						<pre><code data-trim class="scala">
import com.datastax.spark.connector.SomeColumns
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.streaming._
import com.datastax.spark.connector._

val conf = new SparkConf()
   .setAppName("Twitter HashTag Counter")
   .setMaster("local[2]")
   //We need to specify the Cassandra host
   .set("spark.cassandra.connection.host", 
           "127.0.0.1")

val ssc = new StreamingContext(conf, Seconds(1))
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>Note the cassandra configuration</li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Cassandra Stream Word Count (cont...)</h2>

						<span>Next we parse the and filter the tweets</span>

						<pre><code data-trim class="scala">
//Receive all tweets unfiltered
val stream = TwitterUtils
   .createStream(ssc, None)

//Split tweets into words
val words = stream.flatMap(tweet => 
   tweet.getText.toLowerCase.split(" "))

//Pull out the hashtags and map them to a count
val hashtags = words.filter(word => 
   word.startsWith("#"))
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>Not much new here</li>
	          					<li></li>
	          				</ul>
	          			</aside>
					</section>

					<section>
						<h2>Cassandra Stream Word Count (cont...)</h2>

						<span>First we need the appropriate imports and create the context</span>

						<pre><code data-trim class="scala">
//We need to define a case class
case class HashTagCount(
   hashtag: String, count: Long)

hashtags
   //This is exactly the same as the map -> reduce
   .countByValue()
   //map to the case class
   .map(rdd => HashTagCount(rdd._1, rdd._2))
   //save the stream to Cassandra
   .saveToCassandra("spark", "wordcount")
	          			</code></pre>
	          			<aside class="notes">
	          				<ul>
	          					<li>We're saving the case class to cassandra, incrementing the counter</li>
	          					<li>Note that the data is not ordered here</li>
	          				</ul>
	          			</aside>
					</section>

					<section>	
						<h2>Cassandra Stream Word Count output</h2>
						<img class="stretch" src="images/spark-cassandra-wordcount.png"/>
					</section>
				</section>

				<section>
					<h2>That's all folks...</h2>
			        <ul>
			        	<li>Slides at <a href="http://markglh.github.io/BigData-IoT-Presentation-Slides">markglh.github.io/BigData-IoT-Presentation-Slides</a></li>
			        	<li>Code at <a href="https://github.com/markglh/BigData-IoT-Presentation-Examples">github.com/markglh/BigData-IoT-Presentation-Examples</a></li>
			         	<li>Follow me <a href="https://twitter.com/markglh">@markglh</a></li>
			        </ul>
			        <img class="fragment fade-in" src="images/ned-questions.jpg"/>
				</section>
			</div>

		</div>

		<script src="reveal.js/lib/js/head.min.js"></script>
		<script src="reveal.js/js/reveal.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
